{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of try_seg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayelettalby/deep_learning_project/blob/master/Copy_of_try_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw9d53yVlg22",
        "colab_type": "code",
        "outputId": "1e5092e2-f3d8-407f-c1ef-94c20ef72f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip install segmentation_models_pytorch\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from typing import Optional, Union, List\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch import utils\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from PIL import Image\n",
        "import csv\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch.encoders.resnet import resnet_encoders\n",
        "from segmentation_models_pytorch.encoders.dpn import dpn_encoders\n",
        "from segmentation_models_pytorch.encoders.vgg import vgg_encoders\n",
        "from segmentation_models_pytorch.encoders.senet import senet_encoders\n",
        "from segmentation_models_pytorch.encoders.densenet import densenet_encoders\n",
        "from segmentation_models_pytorch.encoders.inceptionresnetv2 import inceptionresnetv2_encoders\n",
        "from segmentation_models_pytorch.encoders.inceptionv4 import inceptionv4_encoders\n",
        "from segmentation_models_pytorch.encoders.efficientnet import efficient_net_encoders\n",
        "from segmentation_models_pytorch.encoders.mobilenet import mobilenet_encoders\n",
        "from segmentation_models_pytorch.encoders.xception import xception_encoders\n",
        "import torch\n",
        "from typing import Optional, Union, List\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from segmentation_models_pytorch.encoders.resnet import resnet_encoders\n",
        "from segmentation_models_pytorch.encoders.dpn import dpn_encoders\n",
        "from segmentation_models_pytorch.encoders.vgg import vgg_encoders\n",
        "from segmentation_models_pytorch.encoders.senet import senet_encoders\n",
        "from segmentation_models_pytorch.encoders.densenet import densenet_encoders\n",
        "from segmentation_models_pytorch.encoders.inceptionresnetv2 import inceptionresnetv2_encoders\n",
        "from segmentation_models_pytorch.encoders.inceptionv4 import inceptionv4_encoders\n",
        "from segmentation_models_pytorch.encoders.efficientnet import efficient_net_encoders\n",
        "from segmentation_models_pytorch.encoders.mobilenet import mobilenet_encoders\n",
        "from segmentation_models_pytorch.encoders.xception import xception_encoders\n",
        "\n",
        "from torch.utils import data\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "\n",
        "encoders = {}\n",
        "encoders.update(resnet_encoders)\n",
        "encoders.update(dpn_encoders)\n",
        "encoders.update(vgg_encoders)\n",
        "encoders.update(senet_encoders)\n",
        "encoders.update(densenet_encoders)\n",
        "encoders.update(inceptionresnetv2_encoders)\n",
        "encoders.update(inceptionv4_encoders)\n",
        "encoders.update(efficient_net_encoders)\n",
        "encoders.update(mobilenet_encoders)\n",
        "encoders.update(xception_encoders)\n",
        "\n",
        "class CenterBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, use_batchnorm):\n",
        "        conv1 = DoubleConvBlock(in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n",
        "                                use_batchnorm=use_batchnorm)\n",
        "        super().__init__(conv1)\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_batchnorm=True, use_transpose_conv=True, mode='nearest'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.use_transpose_conv = use_transpose_conv\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, int(in_channels/2), kernel_size=2, stride=2)\n",
        "        if use_transpose_conv:\n",
        "            self.conv = DoubleConvBlock(int(in_channels/2), out_channels, kernel_size=3, stride=1, padding=1,\n",
        "                                        use_batchnorm=use_batchnorm)\n",
        "        else:\n",
        "            self.conv = DoubleConvBlock(in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n",
        "                                        use_batchnorm=use_batchnorm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_transpose_conv:\n",
        "            x = self.upconv(x)\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=self.mode)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class SegmentationHead(nn.Sequential):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, activation=None, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        if activation is None or activation == 'identity':\n",
        "            self.activation = Identity()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'softmax2d':\n",
        "            self.activation = nn.Softmax(dim=1)\n",
        "        elif activation == 'softmax':\n",
        "            self.activation = nn.Softmax()\n",
        "        elif activation == 'logsoftmax':\n",
        "            self.activation = nn.LogSoftmax()\n",
        "        else:\n",
        "            raise ValueError('Activation should be sigmoid/softmax/logsoftmax/None; got {}'.format(activation))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.activation(x)\n",
        "\n",
        "class UnetDecoder2D(nn.Module):\n",
        "    def __init__(self, encoder_channels, decoder_channels, n_blocks=5, use_batchnorm=True, center=False):\n",
        "        super().__init__()\n",
        "        if n_blocks != len(decoder_channels):\n",
        "            raise ValueError(\"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(n_blocks,\n",
        "                                                                                                           len(decoder_channels)))\n",
        "        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n",
        "        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels)\n",
        "        out_channels = decoder_channels\n",
        "        if center:\n",
        "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
        "        else:\n",
        "            self.center = Identity()\n",
        "        blocks = [DecoderBlock(in_ch, out_ch)\n",
        "            for in_ch, out_ch in zip(in_channels, out_channels)]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, *features):\n",
        "\n",
        "        features = features[1:]  # remove first skip with same spatial resolution\n",
        "        features = features[::-1]  # reverse channels to start from head of encoder\n",
        "\n",
        "        head = features[0]\n",
        "\n",
        "        x = self.center(head)\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            x = decoder_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DoubleConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_batchnorm=True):\n",
        "        super(DoubleConvBlock, self).__init__()\n",
        "        if use_batchnorm:\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                          stride=stride, padding=padding),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                          stride=stride, padding=padding),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True))\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                          stride=stride, padding=padding),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                          stride=stride, padding=padding),\n",
        "                nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class SegmentationModel(torch.nn.Module):\n",
        "\n",
        "    def initialize(self):\n",
        "        self.initialize_decoder(self.decoder)\n",
        "        self.initialize_head(self.segmentation_head)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
        "        features = self.encoder(x)\n",
        "        decoder_output = self.decoder(*features)\n",
        "\n",
        "        masks = self.segmentation_head(decoder_output)\n",
        "\n",
        "        if self.classification_head is not None:\n",
        "            labels = self.classification_head(features[-1])\n",
        "            return masks, labels\n",
        "\n",
        "        return masks\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n",
        "        Args:\n",
        "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
        "        Return:\n",
        "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def initialize_decoder(self, module):\n",
        "        for m in module.modules():\n",
        "\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def initialize_head(self, module):\n",
        "        for m in module.modules():\n",
        "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class Unet_2D(SegmentationModel):\n",
        "    def __init__(self,\n",
        "                 encoder_name: str = \"resnet34\",\n",
        "                 encoder_depth: int = 5,\n",
        "                 encoder_weights: str = \"imagenet\",\n",
        "                 decoder_use_batchnorm: bool = True,\n",
        "                 decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
        "                 in_channels: int = 3,\n",
        "                 classes: int = 1,\n",
        "                 activation: str = 'softmax'):\n",
        "        super(Unet_2D, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.encoder = self.get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth, weights=encoder_weights)\n",
        "\n",
        "        # decoder\n",
        "        self.decoder = UnetDecoder2D(encoder_channels=self.encoder.out_channels, decoder_channels=decoder_channels,\n",
        "                                     n_blocks=encoder_depth, use_batchnorm=decoder_use_batchnorm,\n",
        "                                     center=True if encoder_name.startswith(\"vgg\") else False)\n",
        "\n",
        "        self.segmentation_head = SegmentationHead(in_channels=decoder_channels[-1],\n",
        "                                                  out_channels=classes,\n",
        "                                                  activation=activation,\n",
        "                                                  kernel_size=3)\n",
        "\n",
        "        self.name = 'u-{}'.format(encoder_name)\n",
        "        self.initialize()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        x = self.decoder(*features)\n",
        "        output = self.segmentation_head(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def get_encoder(self, name, in_channels=3, depth=5, weights=None):\n",
        "        Encoder = encoders[name][\"encoder\"]\n",
        "        params = encoders[name][\"params\"]\n",
        "        params.update(depth=depth)\n",
        "        encoder = Encoder(**params)\n",
        "\n",
        "        if weights is not None:\n",
        "            settings = encoders[name][\"pretrained_settings\"][weights]\n",
        "            encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
        "\n",
        "        encoder.set_in_channels(in_channels)\n",
        "\n",
        "        return encoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "model =Unet_2D(encoder_name='efficientnet-b7',\n",
        "                       encoder_depth=5,\n",
        "                       encoder_weights= \"imagenet\",\n",
        "                       decoder_use_batchnorm=\"True\",\n",
        "                       decoder_channels=[256,128,64,32,16],\n",
        "                       in_channels=1,\n",
        "                       classes=1,\n",
        "                       activation='sigmoid')\n",
        "# model=model.double()\n",
        "#model.cuda(0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "path='/content/gdrive/My Drive/Seg_data'\n",
        "x_train_dir=os.path.join(path,'Training')\n",
        "y_train_dir=os.path.join(path,'Training_Lab')\n",
        "x_val_dir=os.path.join(path,'Val')\n",
        "y_val_dir=os.path.join(path, 'Val_Lab')\n",
        "x_test_dir=os.path.join(path,'Test')\n",
        "y_test_dir=os.path.join(path, 'Test_Lab')\n",
        "\n",
        "class Seg_Dataset(BaseDataset):\n",
        "    CLASSES=[0,1]\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            classes=CLASSES,\n",
        "            augmentation=None,\n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_dir=images_dir\n",
        "        self.masks_dir=masks_dir\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        t=os.listdir(self.images_dir)\n",
        "        image = np.load(self.images_dir+'/'+t[i-1])\n",
        "\n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = os.listdir(self.masks_dir)\n",
        "        mask = np.load(self.images_dir+'/'+masks[i-1])\n",
        "\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "train_dataset=Seg_Dataset(x_train_dir,y_train_dir,classes=['0','1'])\n",
        "\n",
        "val_dataset=Seg_Dataset(x_val_dir,y_val_dir,classes=['0','1'])\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
        "\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    optimizer=optimizer,\n",
        "    device=None,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, 10):\n",
        "\n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "    #do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')\n",
        "\n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train: 100%|██████████| 7/7 [03:00<00:00, 25.47s/it, dice_loss - -9.778e-05, iou_score - 1.319]\n",
            "valid: 100%|██████████| 11/11 [00:31<00:00,  2.77s/it, dice_loss - -0.04624, iou_score - 4.896]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Unet_2D. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EfficientNetEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2dStaticSamePadding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ZeroPad2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MBConvBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Identity. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MemoryEfficientSwish. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type UnetDecoder2D. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvTranspose2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DoubleConvBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type SegmentationHead. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sigmoid. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 7/7 [02:54<00:00, 24.85s/it, dice_loss - -0.1211, iou_score - 12.99]\n",
            "valid: 100%|██████████| 11/11 [00:28<00:00,  2.58s/it, dice_loss - -0.09477, iou_score - 16.63]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 7/7 [02:52<00:00, 24.68s/it, dice_loss - -0.1807, iou_score - -236.3]\n",
            "valid: 100%|██████████| 11/11 [00:27<00:00,  2.54s/it, dice_loss - -0.1504, iou_score - 394.1]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train:  14%|█▍        | 1/7 [00:24<02:29, 24.85s/it, dice_loss - -0.2059, iou_score - 71.98]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-592569d8c41a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# update loss logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mbatch_update\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-592569d8c41a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/segmentation_models_pytorch/encoders/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskip_connection_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mskip_connection_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_ratio\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Squeeze and Excitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr560gJJp0P-",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYYo7Knlp0ts",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}